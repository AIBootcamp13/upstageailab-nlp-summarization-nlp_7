bnb:
  bnb_4bit_compute_dtype: float16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true
  load_in_4bit: true
early_stopping:
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
general:
  data_path: data
  model_folder_name: solar_qlora
  model_name: upstage/SOLAR-10.7B-Instruct-v1.0
  output_dir: outputs
inference:
  batch_size: 2
  ckt_path: /root/upstageailab-nlp-summarization-nlp_7/outputs/solar_qlora/checkpoint-1600
  early_stopping: true
  generate_max_length: 128
  no_repeat_ngram_size: 2
  num_beams: 4
  remove_tokens:
  - <usr>
  - <s>
  - </s>
  - <pad>
  result_path: outputs/prediction
lora:
  alpha: 32
  bias: none
  dropout: 0.05
  r: 8
  target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
tokenizer:
  max_length: 1024
  max_prompt_length: 1024
  max_summary_length: 128
  pad_token_as_eos: true
training:
  do_eval: false
  do_train: true
  fp16: true
  gradient_accumulation_steps: 4
  learning_rate: 0.0001
  # load_best_model_at_end: true
  logging_dir: ./logs
  logging_steps: 10
  logging_strategy: steps
  lr_scheduler_type: cosine
  num_train_epochs: 3
  optim: paged_adamw_8bit
  per_device_eval_batch_size: 2
  per_device_train_batch_size: 2
  predict_with_generate: false
  report_to: wandb
  save_safetensors: false
  save_strategy: steps
  save_steps: 50
  save_total_limit: 3
  seed: 42
  warmup_ratio: 0.1
  weight_decay: 0.01
wandb:
  entity: NLP-7
  project: QLoRA
